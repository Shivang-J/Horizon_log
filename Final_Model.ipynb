{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23c472f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import urllib.parse\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler # StandardScaler is for length, though you said normalized, good practice.\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a331759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DistilRoBERTa model and tokenizer...\n",
      "DistilRoBERTa model loaded on: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Global Model and Tokenizer Initialization (for efficiency) ---\n",
    "# Load this once at the start of your script\n",
    "try:\n",
    "    print(\"Loading DistilRoBERTa model and tokenizer...\")\n",
    "    global_tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "    global_model = AutoModel.from_pretrained('distilroberta-base')\n",
    "    global_model.eval() # Set to evaluation mode\n",
    "    global_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    global_model.to(global_device)\n",
    "    print(f\"DistilRoBERTa model loaded on: {global_device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading global model or tokenizer: {e}\")\n",
    "    # Handle error appropriately, e.g., exit or raise\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d9eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. CLS Embedding Extraction Function ---\n",
    "def get_cls_embedding(text: str, tokenizer, model, device, max_length: int = 512):\n",
    "    \"\"\"\n",
    "    Helper function to get [CLS] embedding for a given text.\n",
    "    Handles empty strings by returning a zero vector.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        # Return a zero vector of the correct dimension (768 for DistilRoBERTa-base)\n",
    "        return torch.zeros(model.config.hidden_size).to(device)\n",
    "\n",
    "    encoded_input = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        cls_embedding = model_output.last_hidden_state[0, 0, :]\n",
    "\n",
    "    return cls_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901af503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Main Data Preprocessing Function (FIXED for CUDA Out of Memory)\n",
    "def preprocess_network_logs(df: pd.DataFrame, batch_size: int = 32, max_length: int = 512):\n",
    "    \"\"\"\n",
    "    Preprocesses the network log DataFrame for MLP training,\n",
    "    using batch processing for Transformer embedding generation to mitigate OOM errors.\n",
    "    This function leverages the globally loaded tokenizer, model, and device.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with 'Method', 'length', 'content', 'URL', 'Label' columns.\n",
    "        batch_size (int): The number of samples to process at a time for Transformer inference.\n",
    "                          Adjust this based on your GPU memory. Lower values use less memory.\n",
    "        max_length (int): The maximum sequence length for tokenization.\n",
    "                          Shorter values use less memory.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame ready for MLP training,\n",
    "                      with original 'content' and 'URL' columns dropped.\n",
    "    \"\"\"\n",
    "    # --- Section 1: Prepare Text Data for Batch Processing ---\n",
    "    print(\"Preparing text data for batch processing...\")\n",
    "    # Fill NaN values with empty string for content and URL, then convert to list\n",
    "    # Use .astype(str) before .fillna('') to correctly handle mixed types if any\n",
    "    content_texts = df['content'].fillna('').astype(str).tolist()\n",
    "    # URL-decode here once for all URLs\n",
    "    url_texts = [urllib.parse.unquote(str(url)) for url in df['URL'].fillna('')]\n",
    "\n",
    "    # Initialize lists to store embeddings (will be filled with numpy arrays)\n",
    "    all_content_cls_embeddings = []\n",
    "    all_url_cls_embeddings = []\n",
    "\n",
    "    # --- Section 2: Generate CLS Embeddings in Batches ---\n",
    "    print(f\"Generating [CLS] embeddings in batches (batch_size={batch_size}, max_length={max_length})...\")\n",
    "    num_samples = len(df)\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        # Slice the current batch of texts\n",
    "        batch_content = content_texts[i:i + batch_size]\n",
    "        batch_urls = url_texts[i:i + batch_size]\n",
    "\n",
    "        # Handle empty string inputs for tokenization:\n",
    "        # If a batch element is empty, tokenizer will handle it, but it's crucial\n",
    "        # that the get_cls_embedding for single strings returns zero-vectors if empty.\n",
    "        # When processing a batch, an empty string will result in only CLS and SEP tokens.\n",
    "        # The model will still produce a valid embedding.\n",
    "\n",
    "        # Tokenize current batch of content\n",
    "        # padding='longest' is typically used for batching to pad to the longest sequence in the batch\n",
    "        # truncation=True truncates sequences longer than max_length\n",
    "        encoded_content_batch = global_tokenizer(\n",
    "            batch_content, return_tensors='pt', truncation=True,\n",
    "            padding='longest', max_length=max_length\n",
    "        )\n",
    "        # Move tensors to the global device (GPU/CPU)\n",
    "        encoded_content_batch = {k: v.to(global_device) for k, v in encoded_content_batch.items()}\n",
    "\n",
    "        # Tokenize current batch of URLs\n",
    "        encoded_url_batch = global_tokenizer(\n",
    "            batch_urls, return_tensors='pt', truncation=True,\n",
    "            padding='longest', max_length=max_length\n",
    "        )\n",
    "        # Move tensors to the global device (GPU/CPU)\n",
    "        encoded_url_batch = {k: v.to(global_device) for k, v in encoded_url_batch.items()}\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient calculation for inference\n",
    "            # Get content embeddings\n",
    "            content_output = global_model(**encoded_content_batch)\n",
    "            # Extract CLS token for all items in the batch: [:, 0, :]\n",
    "            # Move to CPU and convert to numpy array immediately to free up GPU memory\n",
    "            batch_content_cls_embeds = content_output.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_content_cls_embeddings.extend(batch_content_cls_embeds)\n",
    "\n",
    "            # Get URL embeddings\n",
    "            url_output = global_model(**encoded_url_batch)\n",
    "            batch_url_cls_embeds = url_output.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_url_cls_embeddings.extend(batch_url_cls_embeds)\n",
    "\n",
    "        # Clear CUDA cache aggressively after each batch (optional, but can help with fragmentation)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Print progress\n",
    "        if (i // batch_size + 1) % 100 == 0 or (i + batch_size) >= num_samples:\n",
    "            print(f\"Processed {min(i + batch_size, num_samples)}/{num_samples} samples...\")\n",
    "\n",
    "    print(\"Embedding generation complete.\")\n",
    "\n",
    "    # Create DataFrames from the accumulated lists of embeddings\n",
    "    content_embed_df = pd.DataFrame(all_content_cls_embeddings, index=df.index).add_prefix('content_embed_')\n",
    "    url_embed_df = pd.DataFrame(all_url_cls_embeddings, index=df.index).add_prefix('url_embed_')\n",
    "\n",
    "\n",
    "    # --- Section 3: One-Hot Encode 'Method' Column ---\n",
    "    print(\"One-hot encoding 'Method' column...\")\n",
    "    method_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "    method_encoded = method_encoder.fit_transform(df[['Method']])\n",
    "    method_encoded_df = pd.DataFrame(\n",
    "        method_encoded,\n",
    "        columns=method_encoder.get_feature_names_out(['Method']),\n",
    "        index=df.index\n",
    "    )\n",
    "    print(\"One-hot encoding complete.\")\n",
    "    joblib.dump(method_encoder, 'method_encoder.pkl')\n",
    "    print(\"OneHotEncoder for 'Method' saved to 'method_encoder.pkl'.\")\n",
    "\n",
    "    # --- Section 4: Combine All Features into Final DataFrame ---\n",
    "    print(\"Combining all features...\")\n",
    "    final_df = df[['length', 'Label']].copy()\n",
    "\n",
    "    final_df = pd.concat([final_df, method_encoded_df], axis=1)\n",
    "    final_df = pd.concat([final_df, content_embed_df], axis=1)\n",
    "    final_df = pd.concat([final_df, url_embed_df], axis=1)\n",
    "\n",
    "    print(\"Feature combination complete.\")\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec75ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original DataFrame head:\n",
      "  Method  length                                            content  \\\n",
      "0    GET     NaN                                                NaN   \n",
      "1    GET     NaN                                                NaN   \n",
      "2   POST    68.0  id=3&nombre=Vino+Rioja&precio=100&cantidad=55&...   \n",
      "3    GET     NaN                                                NaN   \n",
      "4   POST    63.0  modo=entrar&login=choong&pwd=d1se3ci%F3n&remem...   \n",
      "\n",
      "                                                 URL  Label  \n",
      "0   http://localhost:8080/tienda1/index.jsp HTTP/1.1      0  \n",
      "1  http://localhost:8080/tienda1/publico/anadir.j...      0  \n",
      "2  http://localhost:8080/tienda1/publico/anadir.j...      0  \n",
      "3  http://localhost:8080/tienda1/publico/autentic...      0  \n",
      "4  http://localhost:8080/tienda1/publico/autentic...      0  \n",
      "\n",
      "Original DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61065 entries, 0 to 61064\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Method   61065 non-null  object \n",
      " 1   length   17977 non-null  float64\n",
      " 2   content  17977 non-null  object \n",
      " 3   URL      61065 non-null  object \n",
      " 4   Label    61065 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 2.3+ MB\n",
      "\n",
      "Original DataFrame shape: (61065, 5)\n",
      "Preparing text data for batch processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\I'\n",
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_25396\\1946144947.py:2: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  dataset_file_path = \"D:\\Internship ka kaam\\data.csv\\poroject\\Horizon_log\\Final_Data.csv\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating [CLS] embeddings in batches (batch_size=32, max_length=256)...\n",
      "Processed 3200/61065 samples...\n",
      "Processed 6400/61065 samples...\n",
      "Processed 9600/61065 samples...\n",
      "Processed 12800/61065 samples...\n",
      "Processed 16000/61065 samples...\n",
      "Processed 19200/61065 samples...\n",
      "Processed 22400/61065 samples...\n",
      "Processed 25600/61065 samples...\n",
      "Processed 28800/61065 samples...\n",
      "Processed 32000/61065 samples...\n",
      "Processed 35200/61065 samples...\n",
      "Processed 38400/61065 samples...\n",
      "Processed 41600/61065 samples...\n",
      "Processed 44800/61065 samples...\n",
      "Processed 48000/61065 samples...\n",
      "Processed 51200/61065 samples...\n",
      "Processed 54400/61065 samples...\n",
      "Processed 57600/61065 samples...\n",
      "Processed 60800/61065 samples...\n",
      "Processed 61065/61065 samples...\n",
      "Embedding generation complete.\n",
      "One-hot encoding 'Method' column...\n",
      "One-hot encoding complete.\n",
      "OneHotEncoder for 'Method' saved to 'method_encoder.pkl'.\n",
      "Combining all features...\n",
      "Feature combination complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load Your Dataset and Run Preprocessing\n",
    "dataset_file_path = \"D:\\Internship ka kaam\\data.csv\\poroject\\Horizon_log\\Final_Data.csv\"\n",
    "df = pd.read_csv(dataset_file_path)\n",
    "\n",
    "print(\"\\nOriginal DataFrame head:\")\n",
    "print(df.head())\n",
    "print(\"\\nOriginal DataFrame info:\")\n",
    "df.info()\n",
    "print(\"\\nOriginal DataFrame shape:\", df.shape)\n",
    "\n",
    "# Preprocess the DataFrame\n",
    "processed_df = preprocess_network_logs(df.copy(), batch_size=32, max_length=256) # Use .copy() to avoid modifying original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73e0c1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>Label</th>\n",
       "      <th>Method_GET</th>\n",
       "      <th>Method_POST</th>\n",
       "      <th>Method_PUT</th>\n",
       "      <th>content_embed_0</th>\n",
       "      <th>content_embed_1</th>\n",
       "      <th>content_embed_2</th>\n",
       "      <th>content_embed_3</th>\n",
       "      <th>content_embed_4</th>\n",
       "      <th>...</th>\n",
       "      <th>url_embed_758</th>\n",
       "      <th>url_embed_759</th>\n",
       "      <th>url_embed_760</th>\n",
       "      <th>url_embed_761</th>\n",
       "      <th>url_embed_762</th>\n",
       "      <th>url_embed_763</th>\n",
       "      <th>url_embed_764</th>\n",
       "      <th>url_embed_765</th>\n",
       "      <th>url_embed_766</th>\n",
       "      <th>url_embed_767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.028477</td>\n",
       "      <td>0.098757</td>\n",
       "      <td>0.020165</td>\n",
       "      <td>-0.138052</td>\n",
       "      <td>0.102844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019117</td>\n",
       "      <td>-0.018484</td>\n",
       "      <td>-0.057128</td>\n",
       "      <td>-0.044485</td>\n",
       "      <td>0.018033</td>\n",
       "      <td>0.089720</td>\n",
       "      <td>0.089841</td>\n",
       "      <td>-0.106350</td>\n",
       "      <td>-0.042665</td>\n",
       "      <td>0.013191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.028477</td>\n",
       "      <td>0.098757</td>\n",
       "      <td>0.020165</td>\n",
       "      <td>-0.138052</td>\n",
       "      <td>0.102844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019805</td>\n",
       "      <td>-0.007385</td>\n",
       "      <td>-0.098287</td>\n",
       "      <td>-0.075826</td>\n",
       "      <td>0.053666</td>\n",
       "      <td>0.058605</td>\n",
       "      <td>0.026840</td>\n",
       "      <td>-0.079029</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>-0.013870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.042688</td>\n",
       "      <td>-0.032714</td>\n",
       "      <td>-0.080042</td>\n",
       "      <td>0.052732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016956</td>\n",
       "      <td>-0.013940</td>\n",
       "      <td>-0.061787</td>\n",
       "      <td>-0.048854</td>\n",
       "      <td>0.024243</td>\n",
       "      <td>0.080115</td>\n",
       "      <td>0.085346</td>\n",
       "      <td>-0.095609</td>\n",
       "      <td>-0.030425</td>\n",
       "      <td>0.006528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.028477</td>\n",
       "      <td>0.098757</td>\n",
       "      <td>0.020165</td>\n",
       "      <td>-0.138052</td>\n",
       "      <td>0.102844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045856</td>\n",
       "      <td>0.031502</td>\n",
       "      <td>-0.087172</td>\n",
       "      <td>-0.054506</td>\n",
       "      <td>0.066078</td>\n",
       "      <td>0.047454</td>\n",
       "      <td>0.035490</td>\n",
       "      <td>-0.143954</td>\n",
       "      <td>-0.016191</td>\n",
       "      <td>-0.004505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024269</td>\n",
       "      <td>0.072873</td>\n",
       "      <td>-0.019203</td>\n",
       "      <td>-0.063366</td>\n",
       "      <td>0.060539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>-0.012888</td>\n",
       "      <td>-0.052053</td>\n",
       "      <td>-0.046002</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.086654</td>\n",
       "      <td>0.081674</td>\n",
       "      <td>-0.101176</td>\n",
       "      <td>-0.024479</td>\n",
       "      <td>-0.001528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1541 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  Label  Method_GET  Method_POST  Method_PUT  content_embed_0  \\\n",
       "0     0.0      0         1.0          0.0         0.0        -0.028477   \n",
       "1     0.0      0         1.0          0.0         0.0        -0.028477   \n",
       "2    68.0      0         0.0          1.0         0.0         0.023022   \n",
       "3     0.0      0         1.0          0.0         0.0        -0.028477   \n",
       "4    63.0      0         0.0          1.0         0.0         0.024269   \n",
       "\n",
       "   content_embed_1  content_embed_2  content_embed_3  content_embed_4  ...  \\\n",
       "0         0.098757         0.020165        -0.138052         0.102844  ...   \n",
       "1         0.098757         0.020165        -0.138052         0.102844  ...   \n",
       "2         0.042688        -0.032714        -0.080042         0.052732  ...   \n",
       "3         0.098757         0.020165        -0.138052         0.102844  ...   \n",
       "4         0.072873        -0.019203        -0.063366         0.060539  ...   \n",
       "\n",
       "   url_embed_758  url_embed_759  url_embed_760  url_embed_761  url_embed_762  \\\n",
       "0       0.019117      -0.018484      -0.057128      -0.044485       0.018033   \n",
       "1       0.019805      -0.007385      -0.098287      -0.075826       0.053666   \n",
       "2       0.016956      -0.013940      -0.061787      -0.048854       0.024243   \n",
       "3       0.045856       0.031502      -0.087172      -0.054506       0.066078   \n",
       "4       0.016599      -0.012888      -0.052053      -0.046002       0.032960   \n",
       "\n",
       "   url_embed_763  url_embed_764  url_embed_765  url_embed_766  url_embed_767  \n",
       "0       0.089720       0.089841      -0.106350      -0.042665       0.013191  \n",
       "1       0.058605       0.026840      -0.079029       0.002011      -0.013870  \n",
       "2       0.080115       0.085346      -0.095609      -0.030425       0.006528  \n",
       "3       0.047454       0.035490      -0.143954      -0.016191      -0.004505  \n",
       "4       0.086654       0.081674      -0.101176      -0.024479      -0.001528  \n",
       "\n",
       "[5 rows x 1541 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df['length']=processed_df['length'].fillna(0)\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7313fcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn # Neural network modules\n",
    "import torch.optim as optim # Optimization algorithms\n",
    "from torch.utils.data import Dataset, DataLoader # For data handling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib # For saving the trained MLP model and scaler\n",
    "import numpy as np\n",
    "import urllib.parse # Re-import from previous cells if needed globally\n",
    "\n",
    "# Assuming 'processed_df' is already available from the previous steps.\n",
    "# If you are restarting the notebook, ensure you load it:\n",
    "# processed_df = pd.read_parquet('your_processed_df.parquet') # If you saved it as parquet\n",
    "# processed_df = pd.read_csv('your_processed_df.csv') # If you saved it as CSV\n",
    "\n",
    "print(\"Required libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c4066d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing features (X) and labels (y)...\n",
      "Features (X) shape: (61065, 1540)\n",
      "Target (y) shape: (61065,)\n",
      "\n",
      "First 3 rows of Features (X):\n",
      "   length  Method_GET  Method_POST  Method_PUT  content_embed_0  \\\n",
      "0     0.0         1.0          0.0         0.0        -0.028477   \n",
      "1     0.0         1.0          0.0         0.0        -0.028477   \n",
      "2    68.0         0.0          1.0         0.0         0.023022   \n",
      "\n",
      "   content_embed_1  content_embed_2  content_embed_3  content_embed_4  \\\n",
      "0         0.098757         0.020165        -0.138052         0.102844   \n",
      "1         0.098757         0.020165        -0.138052         0.102844   \n",
      "2         0.042688        -0.032714        -0.080042         0.052732   \n",
      "\n",
      "   content_embed_5  ...  url_embed_758  url_embed_759  url_embed_760  \\\n",
      "0        -0.104355  ...       0.019117      -0.018484      -0.057128   \n",
      "1        -0.104355  ...       0.019805      -0.007385      -0.098287   \n",
      "2        -0.101459  ...       0.016956      -0.013940      -0.061787   \n",
      "\n",
      "   url_embed_761  url_embed_762  url_embed_763  url_embed_764  url_embed_765  \\\n",
      "0      -0.044485       0.018033       0.089720       0.089841      -0.106350   \n",
      "1      -0.075826       0.053666       0.058605       0.026840      -0.079029   \n",
      "2      -0.048854       0.024243       0.080115       0.085346      -0.095609   \n",
      "\n",
      "   url_embed_766  url_embed_767  \n",
      "0      -0.042665       0.013191  \n",
      "1       0.002011      -0.013870  \n",
      "2      -0.030425       0.006528  \n",
      "\n",
      "[3 rows x 1540 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Prepare Features (X) and Labels (y) (NO CHANGE)\n",
    "print(\"Preparing features (X) and labels (y)...\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = processed_df.drop('Label', axis=1)\n",
    "y = processed_df['Label']\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "\n",
    "# Display the first few rows of X to confirm its structure\n",
    "print(\"\\nFirst 3 rows of Features (X):\")\n",
    "print(X.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6771db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data into training, validation, and testing sets...\n",
      "Training set shape: (36639, 1540), Labels shape: (36639,)\n",
      "Validation set shape: (12213, 1540), Labels shape: (12213,)\n",
      "Testing set shape: (12213, 1540), Labels shape: (12213,)\n",
      "\n",
      "Class distribution in splits:\n",
      "Train Label counts:\n",
      "Label\n",
      "0    0.589536\n",
      "1    0.410464\n",
      "Name: proportion, dtype: float64\n",
      "Validation Label counts:\n",
      "Label\n",
      "0    0.589536\n",
      "1    0.410464\n",
      "Name: proportion, dtype: float64\n",
      "Test Label counts:\n",
      "Label\n",
      "0    0.589536\n",
      "1    0.410464\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Splitting (Training, Validation, and Testing) (NO CHANGE)\n",
    "print(\"\\nSplitting data into training, validation, and testing sets...\")\n",
    "\n",
    "# First split: Training + Validation (e.g., 80%) and Test (e.g., 20%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: Training (e.g., 75% of X_train_val) and Validation (e.g., 25% of X_train_val)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, Labels shape: {y_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}, Labels shape: {y_val.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}, Labels shape: {y_test.shape}\")\n",
    "\n",
    "# Check class distribution in splits (important for imbalanced datasets)\n",
    "print(\"\\nClass distribution in splits:\")\n",
    "print(f\"Train Label counts:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Validation Label counts:\\n{y_val.value_counts(normalize=True)}\")\n",
    "print(f\"Test Label counts:\\n{y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f2ea77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaling numerical features ('length')...\n",
      "Scaler fitted on training 'length' data.\n",
      "\n",
      "'length_scaler2.pkl' saved. This scaler must be used for new data.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Feature Scaling (MODIFIED - Fitting only)\n",
    "print(\"\\nScaling numerical features ('length')...\")\n",
    "\n",
    "# Create a StandardScaler\n",
    "scaler_for_length = StandardScaler()\n",
    "\n",
    "# Fit the scaler ONLY on the training data's 'length' column\n",
    "# IMPORTANT: We only FIT here. The actual TRANSFORM will happen inside the PyTorch Dataset.\n",
    "scaler_for_length.fit(X_train[['length']])\n",
    "\n",
    "print(\"Scaler fitted on training 'length' data.\")\n",
    "\n",
    "# Save the scaler object for future inference on new data\n",
    "joblib.dump(scaler_for_length, 'length_scaler2.pkl')\n",
    "print(\"\\n'length_scaler2.pkl' saved. This scaler must be used for new data.\")\n",
    "\n",
    "# Note: X_train, X_val, X_test are still original Pandas DataFrames here.\n",
    "# They will be transformed when creating PyTorch Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca553a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Defining PyTorch Dataset and MLP Model ---\n",
      "PyTorch Dataset and SimpleMLP classes defined.\n"
     ]
    }
   ],
   "source": [
    "# NEW CELL: PyTorch Dataset and Model Definitions\n",
    "\n",
    "print(\"\\n--- Defining PyTorch Dataset and MLP Model ---\")\n",
    "\n",
    "# 1. Custom Dataset Class for Tabular Data\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X_df, y_series, scaler_for_length=None):\n",
    "        # Convert DataFrame/Series to numpy arrays first\n",
    "        # Ensure correct order of columns for X based on original processed_df\n",
    "        # Create a consistent column order if not already done, e.g.,\n",
    "        # X_df = X_df[feature_columns] # where feature_columns is a list of your desired column names\n",
    "\n",
    "        # Make a copy to avoid SettingWithCopyWarning during scaling\n",
    "        self.X_data = X_df.copy()\n",
    "        self.y_data = y_series.copy()\n",
    "\n",
    "        self.scaler = scaler_for_length\n",
    "        if self.scaler:\n",
    "            # Apply scaler only to the 'length' column\n",
    "            if 'length' in self.X_data.columns:\n",
    "                self.X_data['length'] = self.scaler.transform(self.X_data[['length']])\n",
    "            else:\n",
    "                print(\"Warning: 'length' column not found in DataFrame for scaling in TabularDataset.\")\n",
    "\n",
    "        # Convert processed numpy arrays to PyTorch tensors\n",
    "        # Ensure float32 for neural network operations\n",
    "        self.X_tensor = torch.tensor(self.X_data.values, dtype=torch.float32)\n",
    "        # Unsqueeze adds a dimension to labels (e.g., [N] -> [N, 1]) needed for BCELoss\n",
    "        self.y_tensor = torch.tensor(self.y_data.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_tensor[idx], self.y_tensor[idx]\n",
    "\n",
    "# 2. Define the MLP Model using torch.nn.Module\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.2):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "\n",
    "        # Create a list of layers\n",
    "        layers = []\n",
    "\n",
    "        # Input layer to first hidden layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout_rate)) # Dropout layer\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate)) # Dropout layer\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        layers.append(nn.Sigmoid()) # Sigmoid for binary classification probability output\n",
    "\n",
    "        # Combine all layers into a sequential model\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"PyTorch Dataset and SimpleMLP classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ae75842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training the PyTorch MLP Classifier ---\n",
      "Using device: cuda\n",
      "Epoch 1/500, Train Loss: 0.4612, Val Loss: 0.2510\n",
      "Epoch 2/500, Train Loss: 0.3042, Val Loss: 0.3119\n",
      "Epoch 3/500, Train Loss: 0.2755, Val Loss: 0.2733\n",
      "Epoch 4/500, Train Loss: 0.2667, Val Loss: 0.2546\n",
      "Epoch 5/500, Train Loss: 0.2928, Val Loss: 0.1936\n",
      "Epoch 6/500, Train Loss: 0.2631, Val Loss: 0.2072\n",
      "Epoch 7/500, Train Loss: 0.2462, Val Loss: 0.1622\n",
      "Epoch 8/500, Train Loss: 0.2419, Val Loss: 0.1860\n",
      "Epoch 9/500, Train Loss: 0.2860, Val Loss: 0.2382\n",
      "Epoch 10/500, Train Loss: 0.2520, Val Loss: 0.1878\n",
      "Epoch 11/500, Train Loss: 0.2339, Val Loss: 0.8461\n",
      "Epoch 12/500, Train Loss: 0.2602, Val Loss: 0.1537\n",
      "Epoch 13/500, Train Loss: 0.2415, Val Loss: 0.1619\n",
      "Epoch 14/500, Train Loss: 0.2704, Val Loss: 0.2062\n",
      "Epoch 15/500, Train Loss: 0.2531, Val Loss: 0.1691\n",
      "Epoch 16/500, Train Loss: 0.2441, Val Loss: 0.1699\n",
      "Epoch 17/500, Train Loss: 0.2586, Val Loss: 0.1967\n",
      "Epoch 18/500, Train Loss: 0.2629, Val Loss: 0.2023\n",
      "Epoch 19/500, Train Loss: 0.2263, Val Loss: 0.1892\n",
      "Epoch 20/500, Train Loss: 0.2725, Val Loss: 0.1595\n",
      "Epoch 21/500, Train Loss: 0.2349, Val Loss: 0.1738\n",
      "Epoch 22/500, Train Loss: 0.2399, Val Loss: 0.1608\n",
      "Early stopping at epoch 22 as validation loss did not improve for 10 epochs.\n",
      "\n",
      "PyTorch MLP training complete.\n",
      "Total training time: 99.19 seconds.\n",
      "Model saved from Best Epoch: 12 with Validation Loss: 0.1537\n"
     ]
    }
   ],
   "source": [
    "# NEW CELL: PyTorch MLP Training Loop (REPLACES old Cell 5)\n",
    "import time # Ensure time is imported\n",
    "\n",
    "print(\"\\n--- Training the PyTorch MLP Classifier ---\")\n",
    "\n",
    "# --- Setup for Training ---\n",
    "# Determine device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create PyTorch Datasets\n",
    "# Pass the fitted scaler to the datasets for consistent 'length' transformation\n",
    "train_dataset = TabularDataset(X_train, y_train, scaler_for_length=scaler_for_length)\n",
    "val_dataset = TabularDataset(X_val, y_val, scaler_for_length=scaler_for_length)\n",
    "test_dataset = TabularDataset(X_test, y_test, scaler_for_length=scaler_for_length)\n",
    "\n",
    "# Define DataLoaders for efficient batching\n",
    "BATCH_SIZE = 64 # Use your desired batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = X_train.shape[1] # Number of features\n",
    "hidden_sizes = (512, 256, 128) # As per your sklearn setup\n",
    "output_size = 1 # For binary classification (one output neuron with sigmoid)\n",
    "DROPOUT_RATE = 0.2 # You can tune this! Common values are 0.1 to 0.5.\n",
    "\n",
    "model = SimpleMLP(input_size, hidden_sizes, output_size, dropout_rate=DROPOUT_RATE).to(device)\n",
    "\n",
    "criterion = nn.BCELoss() # Binary Cross-Entropy Loss for binary classification\n",
    "LEARNING_RATE = 0.001 # Common learning rate for Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- Training Loop with Early Stopping ---\n",
    "NUM_EPOCHS = 500 # Maximum number of epochs\n",
    "N_ITER_NO_CHANGE_PYTORCH = 10 # Number of epochs with no validation loss improvement before stopping\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_epoch = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train() # Set model to training mode (activates dropout)\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Update weights\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval() # Set model to evaluation mode (disables dropout)\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_epoch = epoch + 1\n",
    "        # Save the model's state dictionary (weights and biases)\n",
    "        torch.save(model.state_dict(), 'best_pytorch_mlp_model.pth')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == N_ITER_NO_CHANGE_PYTORCH:\n",
    "            print(f\"Early stopping at epoch {epoch+1} as validation loss did not improve for {N_ITER_NO_CHANGE_PYTORCH} epochs.\")\n",
    "            break\n",
    "\n",
    "end_time = time.time()\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "print(\"\\nPyTorch MLP training complete.\")\n",
    "print(f\"Total training time: {training_duration:.2f} seconds.\")\n",
    "print(f\"Model saved from Best Epoch: {best_epoch} with Validation Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2a46d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating the PyTorch MLP Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_25396\\747657725.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_pytorch_mlp_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation on Test Set (from Best Epoch 12):\n",
      "Test Accuracy: 0.9225\n",
      "Test F1-Score: 0.9027\n",
      "Test ROC AUC: 0.9815\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.95      0.94      7200\n",
      "         1.0       0.93      0.88      0.90      5013\n",
      "\n",
      "    accuracy                           0.92     12213\n",
      "   macro avg       0.92      0.92      0.92     12213\n",
      "weighted avg       0.92      0.92      0.92     12213\n",
      "\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[6871  329]\n",
      " [ 618 4395]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NEW CELL: Evaluate the PyTorch MLP Model (REPLACES old Cell 6)\n",
    "\n",
    "print(\"\\n--- Evaluating the PyTorch MLP Model ---\")\n",
    "\n",
    "# Load the best model's weights\n",
    "# Re-initialize the model with the same architecture\n",
    "model = SimpleMLP(input_size, hidden_sizes, output_size, dropout_rate=DROPOUT_RATE).to(device)\n",
    "model.load_state_dict(torch.load('best_pytorch_mlp_model.pth'))\n",
    "model.eval() # Set to evaluation mode (essential for consistent predictions and disabling dropout)\n",
    "\n",
    "# Collect predictions and true labels for evaluation\n",
    "all_test_preds = []\n",
    "all_test_probas = []\n",
    "all_test_labels = []\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculations for inference\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs) # Get probabilities (output of sigmoid)\n",
    "\n",
    "        # Move to CPU and convert to numpy for sklearn metrics\n",
    "        all_test_probas.extend(outputs.cpu().numpy())\n",
    "        preds = (outputs > 0.5).float() # Threshold probabilities to get binary predictions\n",
    "        all_test_preds.extend(preds.cpu().numpy())\n",
    "        all_test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays for sklearn metrics\n",
    "all_test_preds = np.array(all_test_preds).flatten()\n",
    "all_test_probas = np.array(all_test_probas).flatten()\n",
    "all_test_labels = np.array(all_test_labels).flatten()\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"\\nEvaluation on Test Set (from Best Epoch {best_epoch}):\")\n",
    "print(f\"Test Accuracy: {accuracy_score(all_test_labels, all_test_preds):.4f}\")\n",
    "print(f\"Test F1-Score: {f1_score(all_test_labels, all_test_preds):.4f}\")\n",
    "print(f\"Test ROC AUC: {roc_auc_score(all_test_labels, all_test_probas):.4f}\")\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(all_test_labels, all_test_preds))\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(all_test_labels, all_test_preds))\n",
    "\n",
    "# Plot Confusion Matrix for Test Set\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_matrix(all_test_labels, all_test_preds), annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Normal (0)', 'Malicious (1)'], yticklabels=['Normal (0)', 'Malicious (1)'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - Test Set (PyTorch MLP)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ba0d1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilRoBERTa Tokenizer saved to ./saved_distilroberta_inference\n",
      "DistilRoBERTa Model saved to ./saved_distilroberta_inference\n"
     ]
    }
   ],
   "source": [
    "# Run this once in your training/setup environment if you haven't already:\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Define a directory to save the transformer components\n",
    "TRANSFORMER_LOCAL_PATH = './saved_distilroberta_inference'\n",
    "\n",
    "# Load and save the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "tokenizer.save_pretrained(TRANSFORMER_LOCAL_PATH)\n",
    "print(f\"DistilRoBERTa Tokenizer saved to {TRANSFORMER_LOCAL_PATH}\")\n",
    "\n",
    "# Load and save the model\n",
    "model = AutoModel.from_pretrained('distilroberta-base')\n",
    "model.save_pretrained(TRANSFORMER_LOCAL_PATH)\n",
    "print(f\"DistilRoBERTa Model saved to {TRANSFORMER_LOCAL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
